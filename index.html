<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" integrity="sha256-YvdLHPgkqJ8DVUxjjnGVlMMJtNimJ6dYkowFFvp4kKs=" crossorigin="anonymous">
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@GoogleAI" />
    <meta name="twitter:title" content="Muse: Text-To-Image Generation via Masked Generative Transformers" />
    <meta name="twitter:description" content="Muse: Text-To-Image Generation via Masked Generative Transformers" />
    <meta name="twitter:image" content="https://muse-model.github.io/assets/images/logo/muse_cake_1.jpg" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="./assets/css/styles.css" />

    <script src="https://cdn.jsdelivr.net/npm/jquery@2.2.4/dist/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/jqueryflips@1.0.0/dist/jquery.flip.min.js" integrity="sha256-CAgJ3dV4c4eWFBbunpYeUQZiRlVuR4NsizhWMKgN2L4=" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js" integrity="sha256-cMPWkL3FzjuaFSfEYESYmjF25hCIL6mfRSPnW8OVvM4=" crossorigin="anonymous"></script>

    <title>Muse: Text-To-Image Generation via Masked Generative Transformers</title>
  </head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-V8HJZTYJKX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-V8HJZTYJKX');
  </script>
  <body>

    <div id="gallery0" style="padding-top: 50px; padding-bottom: 50px; background-color: rgb(245,245,245);">
      
      <div class="text-center flipcard" id="fig0">
        <div class="front">
          <figure>
            <img src="./assets/images/logo/muse_cake_1.jpg" style="width: 95%; max-width: 512px;">
            <figcaption>A birthday cake with the word "Muse" written on it.</figcaption>
          </figure>
        </div>
        <div class="back">
          <figure>
            <img src="./assets/images/logo/muse_fire.jpg"  style="width: 95%; max-width: 512px;">
            <figcaption>A fireplace where the flames spell "Muse".</figcaption>
          </figure>
        </div>
      </div>
      
      <h2 style="text-align: center;"><i>Muse:</i> Text-To-Image Generation via Masked Generative Transformers</h2>
    <div class="text-center authors">
      <p>Huiwen Chang<sup>*</sup>, Han Zhang<sup>*</sup>, Jarred Barber<sup>&dagger;</sup>, AJ Maschinot<sup>&dagger;</sup>, Jos√© Lezama, Lu Jiang,<br>  Ming-Hsuan Yang, Kevin Murphy,  William T. Freeman,  Michael Rubinstein<sup>&dagger;</sup>, Yuanzhen Li<sup>&dagger;</sup>, Dilip Krishnan<sup>&dagger;</sup> </p>
      <p class="smaller"><sup>*</sup>Equal contribution. <sup>&dagger;</sup>Core contribution.</p>
      <img src="https://research.google/static/images/google_research_lockup-121987c06c0aa0ab26ca8716a0f2e7945d1cbf82077bbab9f914dac0a0bf099f.svg" alt="Google Research" height=25>
    </div>
  </div>
    <div class="header_dark_gray">
      <h1><i>Muse</i> is a fast, state-of-the-art text-to-image generation and editing model.</h1>
    </div>

    <div class="abstract">
      <div class="inside">
        <p class="text">
        We present <i>Muse</i>, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space:  given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens.  Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality, etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06.  The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model:  inpainting, outpainting, and mask-free editing.
        </p>
        <a class="read-paper" href="https://arxiv.org/abs/2301.00704"><button>Research Paper</button></a>
      </div>
    </div>
    <!-- Block 1: System description -->
    <div class="header_dark_gray">
      <h1>Text-to-image Generation</h1>
      <p class="text-center">Our model generates high-quality images from text prompts <i>fast</i> (1.3s for 512x512 resolution or 0.5s for 256x256 resolution on TPUv4).</p>
    </div>
    <div class="image_8" id="gallery1">
      <div class="flipcard" id="fig0">
        <div class="front">
          <figure>
            <img src="./assets/images/gallery/An abstract, flowery painting_.jpg">
            <figcaption>An abstract, flowery painting</figcaption>
          </figure>
        </div>
        <div class="back">
          <figure>
            <img>
            <figcaption>.</figcaption>
          </figure>
        </div>
      </div>
      <div class="flipcard nomobile" id="fig1">
        <div class="front">
          <figure>
            <img src="./assets/images/gallery/A shallow focus photography of a lamb in a wine glass._1_sr.jpg">
            <figcaption>A shallow focus photography of a lamb in a wine glass.</figcaption>
          </figure>
        </div>
        <div class="back">
          <figure>
            <img />
            <figcaption />
          </figure>
        </div>
      </div>
      <div class="flipcard nomobile" id="fig2">
        <div class="front">
          <figure>
            <img src="./assets/images/gallery/A cake made of macarons in a heart shape_1_sr.jpg">
            <figcaption>A cake made of macarons in a heart shape.</figcaption>
          </figure>
        </div>
        <div class="back">
          <figure>
            <img src="./assets/images/gallery/A cake made of macarons in a heart shape_1_sr.jpg">
            <figcaption>A cake made of macarons in a heart shape.</figcaption>
          </figure>
        </div>
      </div>
      <div class="flipcard nomobile" id="fig3">
        <div class="front">
          <figure>
            <img src="./assets/images/gallery/A tilt shift macro shot of a tiny Christmas town_sr.jpg">
            <figcaption>A tilt shift macro shot of a tiny Christmas town</figcaption>
          </figure>
        </div>
        <div class="back">
          <figure>
            <img src="./assets/images/gallery/A cake made of macarons in a heart shape_1_sr.jpg">
            <figcaption>A cake made of macarons in a heart shape.</figcaption>
          </figure>
        </div>
      </div>
      <div class="flipcard nomobile" id="fig4">
        <div class="front">
          <figure>
            <img src="./assets/images/gallery/An abstract, flowery painting_.jpg">
            <figcaption>An abstract, flowery painting</figcaption>
          </figure>
        </div>
        <div class="back">
          <figure>
            <img src="./assets/images/gallery/An abstract, flowery painting_.jpg">
            <figcaption>.</figcaption>
          </figure>
        </div>
      </div>
      <div class="flipcard nomobile" id="fig5">
        <div class="front">
          <figure>
            <img src="./assets/images/gallery/An abstract, flowery painting_.jpg">
            <figcaption>A shallow focus photography of a lamb in a wine glass.</figcaption>
          </figure>
        </div>
        <div class="back">
          <figure>
            <img />
            <figcaption />
          </figure>
        </div>
      </div>
      <div class="flipcard nomobile" id="fig6">
        <div class="front">
          <figure>
            <img src="./assets/images/gallery/An abstract, flowery painting_.jpg">
            <figcaption>.</figcaption>
          </figure>
        </div>
        <div class="back">
          <figure>
            <img>
            <figcaption>.</figcaption>
          </figure>
        </div>
      </div>
      <div class="flipcard nomobile" id="fig7">
        <div class="front">
          <figure>
            <img src="./assets/images/gallery/An abstract, flowery painting_.jpg">
            <figcaption>A tilt shift macro shot of a tiny Christmas town</figcaption>
          </figure>
        </div>
        <div class="back">
          <figure>
            <img>
            <figcaption>A cake made of macarons in a heart shape.</figcaption>
          </figure>
        </div>
      </div>
    </div>

    <div class="header_dark_gray">
      <h1>Zero-shot, Mask-free editing</h1>
      <p class="text-center">Our model gives us zero-shot, mask-free editing for free by iteratively resampling image tokens conditioned on a text prompt.
    </div>

    <div class="compositional">
      <br>
      <div class="text" id="compositional_madeof">
        <!-- <h3>Choose a prompt:</h3> -->
        <p class="selectable left"><span class="selected">Original</span>
        <!-- <br><span>"A cat wearing a hat."</span> -->
        <br><span>"A cat wearing a tie."</span>
        <br><span>"A dog."</span>
        <br><span>"A pig."</span>
        <br><span>"A shiba inu."</span>
        <br><span>"A rabbit."</span>
        <br><span>"A raccoon."</span>
        <br><span>"A tiger."</span>
        <br><span>"An owl."</span>
        </p>
      </div>
      <br>
      <div class="image">
        <img src="./assets/images/edit_opt/orig.gif" id="compositional_madeof_img">
      </div>
    </div>
    <div class="header_light_gray">
      <h2>Mask-free editing controls multiple objects in an image using only a text prompt.
      </h2>
    </div>
    <div class="edit_grid">
      <div class="caption">A croissant next to a latte with a flower latte art.</div>
      <div>
        <figure>
          <figcaption>Original</figcaption>
          <img src="./assets/images/mfe/18_seed_0_orig_sr.jpg" >
        </figure>
      </div>
      <div>
        <figure>
          <figcaption>Edited</figcaption>
          <img src="./assets/images/mfe/18_seed_0_synth_sr.jpg" >
        </figure>
      </div>
    </div>
    <div class="edit_grid">
      <div class="caption">A bottle of Pinot Grigio next to a glass of white wine and a cork.</div>
      <div>
        <figure>
          <figcaption>Original</figcaption>
          <img src="./assets/images/mfe/68_seed_1_orig_sr.jpg" >
        </figure>
      </div>
      <div>
        <figure>
          <figcaption>Edited</figcaption>
          <img src="./assets/images/mfe/68_seed_1_synth_sr.jpg" >
        </figure>
      </div>
    </div>

    <div class="header_dark_gray">
      <h1>Zero-shot Inpainting/Outpainting</h1>
      <p class="text-center">Our model gives us mask-based editing (inpainting/outpainting) for free: mask-based editing is equivalent to generation.</p>
    </div>

    <div class="compositional">
      <br>
      <div class="text" id="compositional_animals">
        <p class="selectable left"><span class="selected">Original (with mask)</span>
        <br><span>"New York in the background"</span><br>
        <span>"Paris in the background"</span><br>
        <span>"San Francisco in the background"</span>
        </p>
      </div>
      <br>
      <div class="image">
        <img src="./assets/images/inpaint/boston_masked.jpg" id="compositional_animals_img">
      </div>
    </div>
    <div class="edit_grid">
      <div class="caption">Hot air balloons.</div>
      <div>
        <figure>
          <figcaption>Original (+mask)</figcaption>
          <img src="./assets/images/inpaint/greta_orig.jpg">
        </figure>
      </div>
      <div>
        <figure>
          <figcaption>Edited</figcaption>
          <img src="./assets/images/inpaint/greta_balloons.jpg">
        </figure>
      </div>
    </div>
    <div class="edit_grid">
      <div class="caption">Beautiful fall foliage; the gazebo is on a lake</div>
      <div>
        <figure>
          <figcaption>Original (+mask)</figcaption>
          <img src="./assets/images/inpaint/gazebo_masked.jpg">
        </figure>
      </div>
      <div>
        <figure>
          <figcaption>Edited</figcaption>
          <img src="./assets/images/inpaint/gazebo_synth.jpg">
        </figure>
      </div>
    </div>
    <div class="header_dark_gray">
      <h1>Model details</h1>
    </div>
    <div class="white">
      <figure>
        <img src="./assets/images/pipeline_v4.jpg">
        <figcaption><p><i>Muse</i> training pipeline. We feed low resolution and high resolution images into two independent VQGAN tokenizer networks. These tokens are then masked, and low resolution ("base") and high resolution ("superres") transformers are trained to predict masked tokens, conditioned on unmasked tokens and T5 text embeddings.</p></figcaption>
      </figure>
    </div>

    <div class="content1">
      <div>
        <h1>Contributions</h1>
        <ul class="highlights">
          <li>We present a state-of-the-art model for text-to-image generation which achieves excellent FID and CLIP scores, which quantitatively measure image generation quality, diversity and alignment with text prompts.</li>
          <li>Our model is significantly faster than other comparable models due to the use of a compressed, discrete latent space and parallel decoding.</li>
          <li>Our architecture enables out-of-the-box zero-shot capabilities such as inpainting, outpainting, and mask-free image editing.</li>
        </ul>
      </div>
      <div class="right" style="display: flex;">
        <table class="pretty" style="align-self: center;">
          <caption><i>Muse</i> improves inference time over other models.<br>
            <p style="font-size: 0.5em">
              <sup>*</sup> Stable Diffusion times represent the best reported inference times and were not measured internally. The configuration used to achieve FID of 12.63 uses 5x more diffusion steps than what was used for benchmarking, roughly equivalent to 18.5s sampling time. 
            </p>
          </caption>
          <tr style="border-bottom: 3px solid rgb(26, 26, 26);">
            <th class="fid">Model</th>
            <th class="fid">&nbsp;Resolution</th>
            <th class="fid">&nbsp;Inference<br>Time (&darr;)</th>
          </tr>
          <!-- <tr style="border-top: 1px solid rgb(26, 26, 26);"> -->
          <!--   <td>Zero-shot COCO</td> -->
          <!-- </tr> -->
          <tr>
            <td class="model">Stable Diffusion 1.4<sup>*</sup></td>
            <td class="fid">512x512</td>
            <td class="fid">3.7s</td>
          </tr>
          <tr>
            <td class="model">Parti-3B</td>
            <td class="fid">256x256</td>
            <td class="fid">6.4s</td>
          </tr>
          <tr>
            <td class="model">Imagen</td>
            <td class="fid">256x256</td>
            <td class="fid">9.1s</td>
          </tr>
          <tr style="border-bottom: 1px solid ">
            <td class="model">Imagen</td>
            <td class="fid">1024x1024</td>
            <td class="fid">13.3s</td>
          </tr>
          <tr>
            <td class="model" style="font-weight: bold; color: rgb(26, 26, 26);">Muse-3B</td>
            <td class="fid">256x256</td>
            <td class="fid" style="font-weight: bold; color: rgb(26, 26, 26);">0.5s</td>
          </tr>
          </tr>
            <td class="model" style="font-weight: bold; color: rgb(26, 26, 26);">Muse-3B</td>
            <td class="fid">512x512</td>
            <td class="fid" style="font-weight: bold; color: rgb(26, 26, 26);">1.3s</td>
          </tr>
        </table>
      </div>

    </div>
    <div class="content1">
      <div class="right" style="display: flex;">
        <table class="pretty" style="align-self: center;">
          <caption>FID (quality/diversity) and CLIP (text-image alignment) scores measured on zero-shot COCO.</caption>
          <tr style="border-bottom: 3px solid rgb(26, 26, 26);">
            <th class="fid">Model</th>
            <th class="fid">FID (&darr;)</th>
            <th class="fid">&nbsp;CLIP (&uarr;)</th>
          </tr>
          <tr>
            <td class="model">Stable Diffusion 1.4</td>
            <td class="fid">12.63</td>
            <td class="fid">-</td>
          </tr>
          <tr>
            <td class="model">Parti-3B</td>
            <td class="fid">8.10</td>
            <td class="fid">-</td>
          </tr>
          <tr style="border-bottom: 1px solid ">
            <td class="model">Imagen</td>
            <td class="fid" style="font-weight: bold; color: rgb(26, 26, 26);">7.27</td>
            <td class="fid">0.27</td>
          </tr>
          <tr style="border-bottom: 3px solid rgb(26, 26, 26);">
            <td class="model" style="font-weight: bold; color: rgb(26, 26, 26);">Muse-3B</td>
            <td class="fid">7.88</td>
            <td class="fid" style="font-weight: bold; color: rgb(26, 26, 26);">0.32</td>
          </tr>
        </table>
      </div>
    </div>

    <div class="content">
      <h1>Special Thanks</h1>
      <p class="smaller" style="text-align: justify;">We thank William Chan, Chitwan Saharia, and Mohammad Norouzi for providing us training datasets, various evaluation codes, website templates and generous suggestions. Jay Yagnik, Rahul Sukthankar, Tom Duerig and David Salesin provided enthusiastic support of this project for which we are grateful. We thank Victor Gomes and Erica Moreira for infrastructure support, Jing Yu Koh and Jason Baldridge for dataset, model and evaluation discussions and feedback on the paper, Mike Krainin for model speedup discussions, JD Velasquez for discussions and insights, Sarah Laszlo, Kathy Meier-Hellstern, and Rachel Stigler for assisting us with the publication process, Andrew Bunner, Jordi Pont-Tuset, and Shai Noy for help on internal demos, David Fleet, Saurabh Saxena, Jiahui Yu, and Jason Baldridge for sharing Imagen and Parti speed metrics.</p>
    </div>

    <script src="./assets/scripts/index.js"></script>

  </body>
</html>
